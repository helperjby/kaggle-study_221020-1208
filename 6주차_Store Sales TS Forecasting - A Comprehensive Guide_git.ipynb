{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6주차 Store Sales - Time Series Forecasting\n",
    "\n",
    "깃허브 업로드 용량 제한으로 인해 코드 실행결과를 전부 삭제한 버전"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "# PACF - ACF\n",
    "# ------------------------------------------------------\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# DATA VISUALIZATION\n",
    "# ------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# plotly.express https://plotly.com/python-api-reference/plotly.express.html\n",
    "\n",
    "# CONFIGURATIONS\n",
    "# ------------------------------------------------------\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'D:/project/study/kaggle/data/store Sales/'\n",
    "\n",
    "train = pd.read_csv(PATH + \"train.csv\")\n",
    "test = pd.read_csv(PATH + \"test.csv\")\n",
    "stores = pd.read_csv(PATH + \"stores.csv\")\n",
    "#sub = pd.read_csv(PATH + \"sample_submission.csv\")   \n",
    "transactions = pd.read_csv(PATH + \"transactions.csv\").sort_values([\"store_nbr\", \"date\"])\n",
    "\n",
    "\n",
    "# Datetime\n",
    "train[\"date\"] = pd.to_datetime(train.date)\n",
    "test[\"date\"] = pd.to_datetime(test.date)\n",
    "transactions[\"date\"] = pd.to_datetime(transactions.date)\n",
    "\n",
    "# Data types\n",
    "train.onpromotion = train.onpromotion.astype(\"float16\")\n",
    "train.sales = train.sales.astype(\"float32\")\n",
    "stores.cluster = stores.cluster.astype(\"int8\")\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.merge(train.groupby([\"date\", \"store_nbr\"]).sales.sum().reset_index(), transactions, how = \"left\")\n",
    "\n",
    "print(\"Spearman Correlation between Total Sales and Transactions: {:,.4f}\".format(temp.corr(\"spearman\").sales.loc[\"transactions\"]))\n",
    "\n",
    "px.line(transactions.sort_values([\"store_nbr\", \"date\"]), x='date', y='transactions', color='store_nbr', title = \"Transactions\" )\n",
    "\n",
    "# application/vnd.plotly.v1+json \n",
    "# Jupyter Notebook Rdnderers 확장 설치로 해결\n",
    "\n",
    "# groupby\n",
    "# {:,.4f}\".format\n",
    "# corr(\"spearman\")\n",
    "# sales.loc[\"transactions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = transactions.copy()\n",
    "a[\"year\"] = a.date.dt.year\n",
    "a[\"month\"] = a.date.dt.month\n",
    "px.box(a, x=\"year\", y=\"transactions\" , color = \"month\", title = \"Transactions\")\n",
    "\n",
    "# date.dt.year 연도만 남김\n",
    "# date.dt.month month만 남김"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = transactions.set_index(\"date\").resample(\"M\").transactions.mean().reset_index()\n",
    "a[\"year\"] = a.date.dt.year\n",
    "px.line(a, x='date', y='transactions', color='year',title = \"Monthly Average Transactions\" )\n",
    "\n",
    "\n",
    "# set_index = 인덱스로 삼기를 원하는 column을 입력하여 인덱스로 설정\n",
    "# resample = datetime index를 원하는 주기로 나누는 메서드 https://wikidocs.net/158101\n",
    "# df.resample(rule, axis=0, closed=None, label=None, convention='start', kind=None, loffset=None, base=None, on=None, level=None, origin='start_day', offset=None)\n",
    "# (데이터프레임, x축, y축, 색상스타일, 제목)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(temp, x = \"transactions\", y = \"sales\", trendline = \"ols\", trendline_color_override = \"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = transactions.copy()\n",
    "a[\"year\"] = a.date.dt.year\n",
    "a[\"dayofweek\"] = a.date.dt.dayofweek+1\n",
    "a = a.groupby([\"year\", \"dayofweek\"]).transactions.mean().reset_index()\n",
    "px.line(a, x=\"dayofweek\", y=\"transactions\" , color = \"year\", title = \"Transactions\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Oil Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import \n",
    "oil = pd.read_csv(PATH + \"oil.csv\")\n",
    "oil[\"date\"] = pd.to_datetime(oil.date) # date형식의 자료형을 datetime 오브젝트로 변환\n",
    "# Resample\n",
    "oil = oil.set_index(\"date\").dcoilwtico.resample(\"D\").sum().reset_index() # datetime으로 변환된 date를 'D' 기준으로 resample\n",
    "# Interpolate(덧붙이다, 보간하다, (중간값을)채우다)\n",
    "oil[\"dcoilwtico\"] = np.where(oil[\"dcoilwtico\"] == 0, np.nan, oil[\"dcoilwtico\"]) # np.where로 0인 값을 찾으면 np.nan으로 바꾸고 아니면 그대로\n",
    "oil[\"dcoilwtico_interpolated\"] =oil.dcoilwtico.interpolate() # .interpolate() = 시계열데이터의 값에 선형으로 비례하는 방식으로 결측값 보간\n",
    "# Plot\n",
    "p = oil.melt(id_vars=['date']+list(oil.keys()[5:]), var_name='Legend') \n",
    "# pd.melt() id_vars: 기준이 되는 컬럼 지정, value_vars: 녹여서 값과 같이 행으로 들어갈 컬럼\n",
    "# var_name : scalar 변수열에 사용할 이름\n",
    "# oil.keys()?\n",
    "px.line(p.sort_values([\"Legend\", \"date\"], ascending = [False, True]), x='date', y='value', color='Legend',title = \"Daily Oil Price\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.merge(temp, oil, how = \"left\") # temp와 oil 병합\n",
    "print(\"Correlation with Daily Oil Prices\")\n",
    "print(temp.drop([\"store_nbr\", \"dcoilwtico\"], axis = 1).corr(\"spearman\").dcoilwtico_interpolated.loc[[\"sales\", \"transactions\"]], \"\\n\")\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize = (15,5))\n",
    "temp.plot.scatter(x = \"dcoilwtico_interpolated\", y = \"transactions\", ax=axes[0])\n",
    "temp.plot.scatter(x = \"dcoilwtico_interpolated\", y = \"sales\", ax=axes[1], color = \"r\")\n",
    "axes[0].set_title('Daily oil price & Transactions', fontsize = 15)\n",
    "axes[1].set_title('Daily Oil Price & Sales', fontsize = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.merge(train.groupby([\"date\", \"family\"]).sales.sum().reset_index(), oil.drop(\"dcoilwtico\", axis = 1), how = \"left\")\n",
    "# train의 'date'와 'family'를 기준으로 그룹화하는데 'sales'의 합계로 나타내고 인덱스를 리셋한다.\n",
    "# oil에서 'dcoilwtico'를 드랍하고 train와 oil을 병합한다\n",
    "c = a.groupby(\"family\").corr(\"spearman\").reset_index()\n",
    "# a에서 family를 기준으로 그룹화 후 스피어맨 분산을 측정 후 리셋 인덱스\n",
    "c = c[c.level_1 == \"dcoilwtico_interpolated\"][[\"family\", \"sales\"]].sort_values(\"sales\")\n",
    "# ?\n",
    "\n",
    "fig, axes = plt.subplots(7, 5, figsize = (20,20))\n",
    "for i, fam in enumerate(c.family):\n",
    "    if i < 6:\n",
    "        a[a.family == fam].plot.scatter(x = \"dcoilwtico_interpolated\", y = \"sales\", ax=axes[0, i-1])\n",
    "        axes[0, i-1].set_title(fam+\"\\n Correlation:\"+str(c[c.family == fam].sales.iloc[0])[:6], fontsize = 12)\n",
    "        axes[0, i-1].axvline(x=70, color='r', linestyle='--')\n",
    "    if i >= 6 and i<11:\n",
    "        a[a.family == fam].plot.scatter(x = \"dcoilwtico_interpolated\", y = \"sales\", ax=axes[1, i-6])\n",
    "        axes[1, i-6].set_title(fam+\"\\n Correlation:\"+str(c[c.family == fam].sales.iloc[0])[:6], fontsize = 12)\n",
    "        axes[1, i-6].axvline(x=70, color='r', linestyle='--')\n",
    "    if i >= 11 and i<16:\n",
    "        a[a.family == fam].plot.scatter(x = \"dcoilwtico_interpolated\", y = \"sales\", ax=axes[2, i-11])\n",
    "        axes[2, i-11].set_title(fam+\"\\n Correlation:\"+str(c[c.family == fam].sales.iloc[0])[:6], fontsize = 12)\n",
    "        axes[2, i-11].axvline(x=70, color='r', linestyle='--')\n",
    "    if i >= 16 and i<21:\n",
    "        a[a.family == fam].plot.scatter(x = \"dcoilwtico_interpolated\", y = \"sales\", ax=axes[3, i-16])\n",
    "        axes[3, i-16].set_title(fam+\"\\n Correlation:\"+str(c[c.family == fam].sales.iloc[0])[:6], fontsize = 12)\n",
    "        axes[3, i-16].axvline(x=70, color='r', linestyle='--')\n",
    "    if i >= 21 and i<26:\n",
    "        a[a.family == fam].plot.scatter(x = \"dcoilwtico_interpolated\", y = \"sales\", ax=axes[4, i-21])\n",
    "        axes[4, i-21].set_title(fam+\"\\n Correlation:\"+str(c[c.family == fam].sales.iloc[0])[:6], fontsize = 12)\n",
    "        axes[4, i-21].axvline(x=70, color='r', linestyle='--')\n",
    "    if i >= 26 and i < 31:\n",
    "        a[a.family == fam].plot.scatter(x = \"dcoilwtico_interpolated\", y = \"sales\", ax=axes[5, i-26])\n",
    "        axes[5, i-26].set_title(fam+\"\\n Correlation:\"+str(c[c.family == fam].sales.iloc[0])[:6], fontsize = 12)\n",
    "        axes[5, i-26].axvline(x=70, color='r', linestyle='--')\n",
    "    if i >= 31 :\n",
    "        a[a.family == fam].plot.scatter(x = \"dcoilwtico_interpolated\", y = \"sales\", ax=axes[6, i-31])\n",
    "        axes[6, i-31].set_title(fam+\"\\n Correlation:\"+str(c[c.family == fam].sales.iloc[0])[:6], fontsize = 12)\n",
    "        axes[6, i-31].axvline(x=70, color='r', linestyle='--')\n",
    "        \n",
    "        \n",
    "plt.tight_layout(pad=5)\n",
    "plt.suptitle(\"Daily Oil Product & Total Family Sales \\n\", fontsize = 20);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train[[\"store_nbr\", \"sales\"]]\n",
    "a[\"ind\"] = 1\n",
    "a[\"ind\"] = a.groupby(\"store_nbr\").ind.cumsum().values # cumsum() 누적합\n",
    "a = pd.pivot(a, index = \"ind\", columns = \"store_nbr\", values = \"sales\").corr() #\n",
    "mask = np.triu(a.corr()) # np.triu: 상삼각행렬에 해당하는 원소만 남김\n",
    "\n",
    "np.triu(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(a, # 데이터프레임\n",
    "        annot=True, # 각 셀값의 표기 유무\n",
    "        fmt='.1f', # 셀 값의 데이터 타입 설정\n",
    "        cmap='coolwarm', # 컬러 스타일\n",
    "        square=True, # 참인 경우 각 셀이 사각형 모양이 되도록 Axis 측면을 \"같음\"으로 설정\n",
    "        mask=mask, # 결측값이 있는 셀은 자동으로 마스킹됨\n",
    "        linewidths=1, # 각 셀을 분할할 선의 너비입니다.\n",
    "        cbar=False) # 컬러바를 그릴지 말지, 범주\n",
    "plt.title(\"Correlations among stores\",fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train.set_index(\"date\").groupby(\"store_nbr\").resample(\"D\").sales.sum().reset_index()\n",
    "px.line(a, x = \"date\", y= \"sales\", color = \"store_nbr\", title = \"Daily total sales of the stores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "나는 가게들의 시계열을 하나하나 살펴보다가 데이터에 불필요한 행이 몇 개 있다는 것을 깨달았다.  \n",
    "위에서 매장을 선택하면 2013년 초 매출이 없는 곳도 있다. 20, 21, 22, 29, 36, 42, 52, 53번 가게를 보시면 알 수 있습니다.  \n",
    "나는 가게들이 문을 열기 전에 그 줄들을 없애기로 결정했다. 다음 코드에서 제거합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "train = train[~((train.store_nbr == 52) & (train.date < \"2017-04-20\"))]\n",
    "train = train[~((train.store_nbr == 22) & (train.date < \"2015-10-09\"))]\n",
    "train = train[~((train.store_nbr == 42) & (train.date < \"2015-08-21\"))]\n",
    "train = train[~((train.store_nbr == 21) & (train.date < \"2015-07-24\"))]\n",
    "train = train[~((train.store_nbr == 29) & (train.date < \"2015-03-20\"))]\n",
    "train = train[~((train.store_nbr == 20) & (train.date < \"2015-02-13\"))]\n",
    "train = train[~((train.store_nbr == 53) & (train.date < \"2014-05-29\"))]\n",
    "train = train[~((train.store_nbr == 36) & (train.date < \"2013-05-09\"))]\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Forecasting\n",
    "\n",
    "일부 상점에서는 일부 제품군을 판매하지 않습니다. 다음 코드에서 어떤 제품이 어떤 상점에서 판매되지 않는지 확인할 수 있습니다.  \n",
    "다음 15일 동안 그것들을 예측하는 것은 어렵지 않다. 그들의 예측은 다음 15일 동안 0이어야 한다.  \n",
    "데이터에서 제거하고 판매하지 않는 제품군을 위한 새로운 데이터 프레임을 만들 것입니다.  \n",
    "그러면 제출 부분에 있을 때 그 데이터 프레임을 우리의 예측과 결합하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = train.groupby([\"store_nbr\", \"family\"]).sales.sum().reset_index().sort_values([\"family\",\"store_nbr\"]) # family와 store_nbr을 기준으로 오름차순\n",
    "c = c[c.sales == 0] # sales값이 0인것만\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "# Anti Join\n",
    "outer_join = train.merge(c[c.sales == 0].drop(\"sales\",axis = 1), how = 'outer', indicator = True)\n",
    "train = outer_join[~(outer_join._merge == 'both')].drop('_merge', axis = 1) # _merge\n",
    "\n",
    "del outer_join # 객체를 메모리에서 제거\n",
    "gc.collect() # gc.collect()가 반환하는 값은 점유된 객체 숫자와 메모리가 해제된 객체 숫자를 의미한다.?\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_prediction = []\n",
    "for i in range(0,len(c)):\n",
    "    zero_prediction.append(\n",
    "        pd.DataFrame({\n",
    "            \"date\":pd.date_range(\"2017-08-16\", \"2017-08-31\").tolist(), # tolist() : 같은 레벨에 있는 데이터끼리 묶어준다\n",
    "            \"store_nbr\":c.store_nbr.iloc[i],\n",
    "            \"family\":c.family.iloc[i],\n",
    "            \"sales\":0\n",
    "        })\n",
    "    )\n",
    "zero_prediction = pd.concat(zero_prediction) # 데이터프레임 생성\n",
    "del c \n",
    "gc.collect()\n",
    "zero_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are The Product Families Active or Passive?\n",
    "\n",
    "어떤 제품들은 가게에서 거의 팔리지 않는다.  \n",
    "이전 직장에서 식당 프로젝트를 위한 제품 공급 수요에 대해 작업했을 때, 일부 제품은 지난 두 달 동안 한 번도 구매하지 않으면 소극적이었습니다.   \n",
    "이 도메인 지식을 여기에 적용하고 마지막 60일을 살펴보겠습니다.  \n",
    "\n",
    "그러나 일부 제품군은 계절성에 따라 달라집니다.   \n",
    "그들 중 일부는 지난 60일 동안 활동하지 않을 수 있지만 그것이 수동적이라는 것을 의미하지는 않는다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = train.groupby([\"family\", \"store_nbr\"]).tail(60).groupby([\"family\", \"store_nbr\"]).sales.sum().reset_index()\n",
    "c[c.sales == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래에서 보시는 것처럼 이런 사례가 너무 드물고 매출도 저조한 상황입니다.   \n",
    "이 가족들을 위해 제안을 해주시면 감사하겠습니다. 지금은 아무것도 하지 않겠지만, 당신은 당신의 모델을 향상시키고 싶어한다.   \n",
    "당신은 그것에 집중할 수 있다  \n",
    "\n",
    "하지만 여전히, 나는 그것이 간단한지 그 지식을 사용하고 싶고 새로운 기능을 만들 것이다. 제품군이 활성 상태인지 여부를 나타냅니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,5, figsize = (20,4))\n",
    "train[(train.store_nbr == 10) & (train.family == \"LAWN AND GARDEN\")].set_index(\"date\").sales.plot(ax = ax[0], title = \"STORE 10 - LAWN AND GARDEN\")\n",
    "train[(train.store_nbr == 36) & (train.family == \"LADIESWEAR\")].set_index(\"date\").sales.plot(ax = ax[1], title = \"STORE 36 - LADIESWEAR\")\n",
    "train[(train.store_nbr == 6) & (train.family == \"SCHOOL AND OFFICE SUPPLIES\")].set_index(\"date\").sales.plot(ax = ax[2], title = \"STORE 6 - SCHOOL AND OFFICE SUPPLIES\")\n",
    "train[(train.store_nbr == 14) & (train.family == \"BABY CARE\")].set_index(\"date\").sales.plot(ax = ax[3], title = \"STORE 14 - BABY CARE\")\n",
    "train[(train.store_nbr == 53) & (train.family == \"BOOKS\")].set_index(\"date\").sales.plot(ax = ax[4], title = \"STORE 43 - BOOKS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train.set_index(\"date\").groupby(\"family\").resample(\"D\").sales.sum().reset_index()\n",
    "px.line(a, x = \"date\", y= \"sales\", color = \"family\", title = \"Daily total sales of the family\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 상점들과 함께 일하고 있다. 글쎄요, 매장에 많은 제품이 있는데 어떤 제품군이 더 많이 팔리는지 알아야 하나요? 그것을 보기 위해 막대 그래프를 만들어 봅시다.\n",
    "\n",
    "그래프는 GROCERY I와 BEVERAGES가 가장 잘 팔리는 가족임을 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train.groupby(\"family\").sales.mean().sort_values(ascending = False).reset_index()\n",
    "px.bar(a, y = \"family\", x=\"sales\", color = \"family\", title = \"Which product family preferred more?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Spearman Correlation between Sales and Onpromotion: {:,.4f}\".format(train.corr(\"spearman\").sales.loc[\"onpromotion\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상점들은 서로 얼마나 다를 수 있는가?  \n",
    "나는 실제로 가게들 사이에서 주요 패턴을 찾을 수 없었다.  \n",
    "하지만 나는 단 하나의 줄거리만을 보았다. 잠재된 패턴이 있을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.merge(train, stores)\n",
    "d[\"store_nbr\"] = d[\"store_nbr\"].astype(\"int8\")\n",
    "d[\"year\"] = d.date.dt.year\n",
    "px.line(d.groupby([\"city\", \"year\"]).sales.mean().reset_index(), x = \"year\", y = \"sales\", color = \"city\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Holidays and Events\n",
    "\n",
    "What are our problems?\n",
    "* Some national holidays have been transferred.\n",
    "* There might be a few holidays in one day. When we merged all of data, number of rows might increase. We don't want duplicates.\n",
    "* What is the scope of holidays? It can be regional or national or local. You need to split them by the scope.\n",
    "* Work day issue\n",
    "* Some specific events\n",
    "* Creating new features etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays = pd.read_csv(PATH + \"holidays_events.csv\")\n",
    "holidays[\"date\"] = pd.to_datetime(holidays.date)\n",
    "\n",
    "# holidays[holidays.type == \"Holiday\"]\n",
    "# holidays[(holidays.type == \"Holiday\") & (holidays.transferred == True)]\n",
    "\n",
    "# Transferred Holidays\n",
    "tr1 = holidays[(holidays.type == \"Holiday\") & (holidays.transferred == True)].drop(\"transferred\", axis = 1).reset_index(drop = True)\n",
    "tr2 = holidays[(holidays.type == \"Transfer\")].drop(\"transferred\", axis = 1).reset_index(drop = True)\n",
    "tr = pd.concat([tr1,tr2], axis = 1)\n",
    "tr3 = tr.iloc[:, [5,1,2,3,4]] # 5,1,2,3,4 열만\n",
    "\n",
    "holidays = holidays[(holidays.transferred == False) & (holidays.type != \"Transfer\")].drop(\"transferred\", axis = 1)\n",
    "holidays = holidays.append(tr3).reset_index(drop = True)\n",
    "\n",
    "holidays2 = pd.read_csv(PATH + \"holidays_events.csv\")\n",
    "\n",
    "holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays2[\"description\"].unique()\n",
    "h = holidays2[\"description\"].str.replace(\"-\", \"\")\n",
    "h2 = h.str.replace(\"+\", \"\")\n",
    "h3 = h2.str.replace('\\d+', '')\n",
    "print(h)\n",
    "print(h2)\n",
    "print(h3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Additional Holidays\n",
    "holidays[\"description\"] = holidays[\"description\"].str.replace(\"-\", \"\").str.replace(\"+\", \"\").str.replace('\\d+', '')\n",
    "# '\\d+' 하나 혹은 그 이상 연결된 숫자\n",
    "holidays[\"type\"] = np.where(holidays[\"type\"] == \"Additional\", \"Holiday\", holidays[\"type\"])\n",
    "# 에디셔널이면 홀리데이로 바꾸고 아니면 그대로 둬라\n",
    "\n",
    "# Bridge Holidays\n",
    "holidays[\"description\"] = holidays[\"description\"].str.replace(\"Puente \", \"\")\n",
    "holidays[\"type\"] = np.where(holidays[\"type\"] == \"Bridge\", \"Holiday\", holidays[\"type\"])\n",
    "\n",
    " \n",
    "# Work Day Holidays, that is meant to payback the Bridge.\n",
    "work_day = holidays[holidays.type == \"Work Day\"]  \n",
    "holidays = holidays[holidays.type != \"Work Day\"]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "# Events are national\n",
    "events = holidays[holidays.type == \"Event\"].drop([\"type\", \"locale\", \"locale_name\"], axis = 1).rename({\"description\":\"events\"}, axis = 1)\n",
    "# holidays.type의 값이 event인 것에서 'type, locale, locale_name을 드롭하고, 컬럼명을 events로 바꾼다\n",
    "\n",
    "holidays = holidays[holidays.type != \"Event\"].drop(\"type\", axis = 1)\n",
    "regional = holidays[holidays.locale == \"Regional\"].rename({\"locale_name\":\"state\", \"description\":\"holiday_regional\"}, axis = 1).drop(\"locale\", axis = 1).drop_duplicates() # 중복행 제거\n",
    "national = holidays[holidays.locale == \"National\"].rename({\"description\":\"holiday_national\"}, axis = 1).drop([\"locale\", \"locale_name\"], axis = 1).drop_duplicates()\n",
    "local = holidays[holidays.locale == \"Local\"].rename({\"description\":\"holiday_local\", \"locale_name\":\"city\"}, axis = 1).drop(\"locale\", axis = 1).drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events\n",
    "holidays\n",
    "regional\n",
    "national\n",
    "local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.merge(train.append(test), stores)\n",
    "d[\"store_nbr\"] = d[\"store_nbr\"].astype(\"int8\")\n",
    "\n",
    "# National Holidays & Events\n",
    "#d = pd.merge(d, events, how = \"left\")\n",
    "d = pd.merge(d, national, how = \"left\")\n",
    "# Regional\n",
    "d = pd.merge(d, regional, how = \"left\", on = [\"date\", \"state\"])\n",
    "# Local\n",
    "d = pd.merge(d, local, how = \"left\", on = [\"date\", \"city\"])\n",
    "\n",
    "# Work Day: It will be removed when real work day colum created\n",
    "d = pd.merge(d,  work_day[[\"date\", \"type\"]].rename({\"type\":\"IsWorkDay\"}, axis = 1), how = \"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVENTS\n",
    "events[\"events\"] =np.where(events.events.str.contains(\"futbol\"), \"Futbol\", events.events)\n",
    "\n",
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    original_columns = list(df.columns) # df가 앞에서 정의되지 않았는데 이게 되나?\n",
    "    categorical_columns = df.select_dtypes([\"category\", \"object\"]).columns.tolist()\n",
    "    # categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category) \n",
    "    # columns: one-hot 인코딩 할 컬럼 dummy_na: NaN도 DUMMY VARIABLE에 포함시킬 지 여부\n",
    "    new_columns = [c for c in df.columns if c not in original_columns] \n",
    "    # original_columns 안에 c가 없다면 참이다\n",
    "    df.columns = df.columns.str.replace(\" \", \"_\")\n",
    "    return df, df.columns.tolist()\n",
    "\n",
    "\n",
    "\n",
    "events, events_cat = one_hot_encoder(events, nan_as_category=False)\n",
    "events[\"events_Dia_de_la_Madre\"] = np.where(events.date == \"2016-05-08\", 1, events[\"events_Dia_de_la_Madre\"])\n",
    "events = events.drop(239)\n",
    "\n",
    "d = pd.merge(d, events, how = \"left\")\n",
    "d[events_cat] = d[events_cat].fillna(0) # 결측값 0으로 채우기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New features\n",
    "d[\"holiday_national_binary\"] = np.where(d.holiday_national.notnull(), 1, 0) # notnull(): 결측값이면 false 반환, false면 1로 바꾸고 아니면 0\n",
    "d[\"holiday_local_binary\"] = np.where(d.holiday_local.notnull(), 1, 0)\n",
    "d[\"holiday_regional_binary\"] = np.where(d.holiday_regional.notnull(), 1, 0)\n",
    "\n",
    "# \n",
    "d[\"national_independence\"] = np.where(d.holiday_national.isin(['Batalla de Pichincha',  'Independencia de Cuenca', 'Independencia de Guayaquil', 'Independencia de Guayaquil', 'Primer Grito de Independencia']), 1, 0)\n",
    "d[\"local_cantonizacio\"] = np.where(d.holiday_local.str.contains(\"Cantonizacio\"), 1, 0)\n",
    "d[\"local_fundacion\"] = np.where(d.holiday_local.str.contains(\"Fundacion\"), 1, 0)\n",
    "d[\"local_independencia\"] = np.where(d.holiday_local.str.contains(\"Independencia\"), 1, 0)\n",
    "\n",
    "\n",
    "holidays, holidays_cat = one_hot_encoder(d[[\"holiday_national\",\"holiday_regional\",\"holiday_local\"]], nan_as_category=False)\n",
    "d = pd.concat([d.drop([\"holiday_national\",\"holiday_regional\",\"holiday_local\"], axis = 1),holidays], axis = 1)\n",
    "\n",
    "he_cols = d.columns[d.columns.str.startswith(\"events\")].tolist() + d.columns[d.columns.str.startswith(\"holiday\")].tolist() + d.columns[d.columns.str.startswith(\"national\")].tolist()+ d.columns[d.columns.str.startswith(\"local\")].tolist()\n",
    "d[he_cols] = d[he_cols].astype(\"int8\")\n",
    "\n",
    "d[[\"family\", \"city\", \"state\", \"type\"]] = d[[\"family\", \"city\", \"state\", \"type\"]].astype(\"category\")\n",
    "\n",
    "del holidays, holidays_cat, work_day, local, regional, national, events, events_cat, tr, tr1, tr2, he_cols\n",
    "gc.collect()\n",
    "\n",
    "d.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AB_Test(dataframe, group, target):\n",
    "    \n",
    "    # Packages\n",
    "    from scipy.stats import shapiro\n",
    "    import scipy.stats as stats\n",
    "    \n",
    "    # Split A/B\n",
    "    groupA = dataframe[dataframe[group] == 1][target]\n",
    "    groupB = dataframe[dataframe[group] == 0][target]\n",
    "    \n",
    "    # Assumption: Normality\n",
    "    ntA = shapiro(groupA)[1] < 0.05\n",
    "    ntB = shapiro(groupB)[1] < 0.05\n",
    "    # H0: Distribution is Normal! - False\n",
    "    # H1: Distribution is not Normal! - True\n",
    "    \n",
    "    if (ntA == False) & (ntB == False): # \"H0: Normal Distribution\"\n",
    "        # Parametric Test\n",
    "        # Assumption: Homogeneity of variances\n",
    "        leveneTest = stats.levene(groupA, groupB)[1] < 0.05\n",
    "        # H0: Homogeneity: False\n",
    "        # H1: Heterogeneous: True\n",
    "        \n",
    "        if leveneTest == False:\n",
    "            # Homogeneity\n",
    "            ttest = stats.ttest_ind(groupA, groupB, equal_var=True)[1]\n",
    "            # H0: M1 == M2 - False\n",
    "            # H1: M1 != M2 - True\n",
    "        else:\n",
    "            # Heterogeneous\n",
    "            ttest = stats.ttest_ind(groupA, groupB, equal_var=False)[1]\n",
    "            # H0: M1 == M2 - False\n",
    "            # H1: M1 != M2 - True\n",
    "    else:\n",
    "        # Non-Parametric Test\n",
    "        ttest = stats.mannwhitneyu(groupA, groupB)[1] \n",
    "        # H0: M1 == M2 - False\n",
    "        # H1: M1 != M2 - True\n",
    "        \n",
    "    # Result\n",
    "    temp = pd.DataFrame({\n",
    "        \"AB Hypothesis\":[ttest < 0.05], \n",
    "        \"p-value\":[ttest]\n",
    "    })\n",
    "    temp[\"Test Type\"] = np.where((ntA == False) & (ntB == False), \"Parametric\", \"Non-Parametric\")\n",
    "    temp[\"AB Hypothesis\"] = np.where(temp[\"AB Hypothesis\"] == False, \"Fail to Reject H0\", \"Reject H0\")\n",
    "    temp[\"Comment\"] = np.where(temp[\"AB Hypothesis\"] == \"Fail to Reject H0\", \"A/B groups are similar!\", \"A/B groups are not similar!\")\n",
    "    temp[\"Feature\"] = group\n",
    "    temp[\"GroupA_mean\"] = groupA.mean()\n",
    "    temp[\"GroupB_mean\"] = groupB.mean()\n",
    "    temp[\"GroupA_median\"] = groupA.median()\n",
    "    temp[\"GroupB_median\"] = groupB.median()\n",
    "    \n",
    "    # Columns\n",
    "    if (ntA == False) & (ntB == False):\n",
    "        temp[\"Homogeneity\"] = np.where(leveneTest == False, \"Yes\", \"No\")\n",
    "        temp = temp[[\"Feature\",\"Test Type\", \"Homogeneity\",\"AB Hypothesis\", \"p-value\", \"Comment\", \"GroupA_mean\", \"GroupB_mean\", \"GroupA_median\", \"GroupB_median\"]]\n",
    "    else:\n",
    "        temp = temp[[\"Feature\",\"Test Type\",\"AB Hypothesis\", \"p-value\", \"Comment\", \"GroupA_mean\", \"GroupB_mean\", \"GroupA_median\", \"GroupB_median\"]]\n",
    "    \n",
    "    # Print Hypothesis\n",
    "    # print(\"# A/B Testing Hypothesis\")\n",
    "    # print(\"H0: A == B\")\n",
    "    # print(\"H1: A != B\", \"\\n\")\n",
    "    \n",
    "    return temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply A/B Testing\n",
    "he_cols = d.columns[d.columns.str.startswith(\"events\")].tolist() + d.columns[d.columns.str.startswith(\"holiday\")].tolist() + d.columns[d.columns.str.startswith(\"national\")].tolist()+ d.columns[d.columns.str.startswith(\"local\")].tolist()\n",
    "ab = []\n",
    "for i in he_cols:\n",
    "    ab.append(AB_Test(dataframe=d[d.sales.notnull()], group = i, target = \"sales\"))\n",
    "ab = pd.concat(ab)\n",
    "ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.groupby([\"family\",\"events_Futbol\"]).sales.mean()[:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Time Related Features\n",
    "How many features can you create from only date column? I'm sharing an example of time related features. You can expand the features with your imagination or your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Related Features\n",
    "def create_date_features(df):\n",
    "    df['month'] = df.date.dt.month.astype(\"int8\")\n",
    "    df['day_of_month'] = df.date.dt.day.astype(\"int8\")\n",
    "    df['day_of_year'] = df.date.dt.dayofyear.astype(\"int16\")\n",
    "    df['week_of_month'] = (df.date.apply(lambda d: (d.day-1) // 7 + 1)).astype(\"int8\")\n",
    "    df['week_of_year'] = (df.date.dt.weekofyear).astype(\"int8\")\n",
    "    df['day_of_week'] = (df.date.dt.dayofweek + 1).astype(\"int8\")\n",
    "    df['year'] = df.date.dt.year.astype(\"int32\")\n",
    "    df[\"is_wknd\"] = (df.date.dt.weekday // 4).astype(\"int8\")\n",
    "    df[\"quarter\"] = df.date.dt.quarter.astype(\"int8\")\n",
    "    df['is_month_start'] = df.date.dt.is_month_start.astype(\"int8\")\n",
    "    df['is_month_end'] = df.date.dt.is_month_end.astype(\"int8\")\n",
    "    df['is_quarter_start'] = df.date.dt.is_quarter_start.astype(\"int8\")\n",
    "    df['is_quarter_end'] = df.date.dt.is_quarter_end.astype(\"int8\")\n",
    "    df['is_year_start'] = df.date.dt.is_year_start.astype(\"int8\")\n",
    "    df['is_year_end'] = df.date.dt.is_year_end.astype(\"int8\")\n",
    "    # 0: Winter - 1: Spring - 2: Summer - 3: Fall\n",
    "    df[\"season\"] = np.where(df.month.isin([12,1,2]), 0, 1)\n",
    "    df[\"season\"] = np.where(df.month.isin([6,7,8]), 2, df[\"season\"])\n",
    "    df[\"season\"] = pd.Series(np.where(df.month.isin([9, 10, 11]), 3, df[\"season\"])).astype(\"int8\")\n",
    "    return df\n",
    "d = create_date_features(d)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Workday column\n",
    "d[\"workday\"] = np.where((d.holiday_national_binary == 1) | (d.holiday_local_binary==1) | (d.holiday_regional_binary==1) | (d['day_of_week'].isin([6,7])), 0, 1)\n",
    "d[\"workday\"] = pd.Series(np.where(d.IsWorkDay.notnull(), 1, d[\"workday\"])).astype(\"int8\")\n",
    "d.drop(\"IsWorkDay\", axis = 1, inplace = True)\n",
    "\n",
    "# Wages in the public sector are paid every two weeks on the 15 th and on the last day of the month. \n",
    "# Supermarket sales could be affected by this.\n",
    "d[\"wageday\"] = pd.Series(np.where((d['is_month_end'] == 1) | (d[\"day_of_month\"] == 15), 1, 0)).astype(\"int8\")\n",
    "\n",
    "d.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Did Earhquake affect the store sales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[(d.month.isin([4,5]))].groupby([\"year\"]).sales.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "March"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(d[(d.month.isin([3]))], index=\"year\", columns=\"family\", values=\"sales\", aggfunc=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "April - May"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(d[(d.month.isin([4,5]))], index=\"year\", columns=\"family\", values=\"sales\", aggfunc=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "June"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(d[(d.month.isin([6]))], index=\"year\", columns=\"family\", values=\"sales\", aggfunc=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. ACF & PACF for each family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = d[d[\"store_nbr\"]==1].set_index(\"date\")\n",
    "a = d[(d.sales.notnull())].groupby([\"date\", \"family\"]).sales.mean().reset_index().set_index(\"date\")\n",
    "for num, i in enumerate(a.family.unique()):\n",
    "    try:\n",
    "        fig, ax = plt.subplots(1,2,figsize=(15,5))\n",
    "        temp = a[(a.family == i)]#& (a.sales.notnull())\n",
    "        sm.graphics.tsa.plot_acf(temp.sales, lags=365, ax=ax[0], title = \"AUTOCORRELATION\\n\" + i)\n",
    "        sm.graphics.tsa.plot_pacf(temp.sales, lags=365, ax=ax[1], title = \"PARTIAL AUTOCORRELATION\\n\" + i)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = d[d.year.isin([2016,2017])].groupby([\"year\", \"day_of_year\"]).sales.mean().reset_index()\n",
    "px.line(a, x = \"day_of_year\", y = \"sales\", color = \"year\", title = \"Average sales for 2016 and 2017\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Simple Moving Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train.sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "for i in [20, 30, 45, 60, 90, 120, 365, 730]:\n",
    "    a[\"SMA\"+str(i)+\"_sales_lag16\"] = a.groupby([\"store_nbr\", \"family\"]).rolling(i).sales.mean().shift(16).values\n",
    "    a[\"SMA\"+str(i)+\"_sales_lag30\"] = a.groupby([\"store_nbr\", \"family\"]).rolling(i).sales.mean().shift(30).values\n",
    "    a[\"SMA\"+str(i)+\"_sales_lag60\"] = a.groupby([\"store_nbr\", \"family\"]).rolling(i).sales.mean().shift(60).values\n",
    "print(\"Correlation\")\n",
    "a[[\"sales\"]+a.columns[a.columns.str.startswith(\"SMA\")].tolist()].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a[(a.store_nbr == 1)].set_index(\"date\")\n",
    "for i in b.family.unique():\n",
    "    fig, ax = plt.subplots(2, 4, figsize=(20,10))\n",
    "    b[b.family == i][[\"sales\", \"SMA20_sales_lag16\"]].plot(legend = True, ax = ax[0,0], linewidth = 4)\n",
    "    b[b.family == i][[\"sales\", \"SMA30_sales_lag16\"]].plot(legend = True, ax = ax[0,1], linewidth = 4)\n",
    "    b[b.family == i][[\"sales\", \"SMA45_sales_lag16\"]].plot(legend = True, ax = ax[0,2], linewidth = 4)\n",
    "    b[b.family == i][[\"sales\", \"SMA60_sales_lag16\"]].plot(legend = True, ax = ax[0,3], linewidth = 4)\n",
    "    b[b.family == i][[\"sales\", \"SMA90_sales_lag16\"]].plot(legend = True, ax = ax[1,0], linewidth = 4)\n",
    "    b[b.family == i][[\"sales\", \"SMA120_sales_lag16\"]].plot(legend = True, ax = ax[1,1], linewidth = 4)\n",
    "    b[b.family == i][[\"sales\", \"SMA365_sales_lag16\"]].plot(legend = True, ax = ax[1,2], linewidth = 4)\n",
    "    b[b.family == i][[\"sales\", \"SMA730_sales_lag16\"]].plot(legend = True, ax = ax[1,3], linewidth = 4)\n",
    "    plt.suptitle(\"STORE 1 - \"+i, fontsize = 15)\n",
    "    plt.tight_layout(pad = 1.5)\n",
    "    for j in range(0,4):\n",
    "        ax[0,j].legend(fontsize=\"x-large\")\n",
    "        ax[1,j].legend(fontsize=\"x-large\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Exponential Moving Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ewm_features(dataframe, alphas, lags):\n",
    "    dataframe = dataframe.copy()\n",
    "    for alpha in alphas:\n",
    "        for lag in lags:\n",
    "            dataframe['sales_ewm_alpha_' + str(alpha).replace(\".\", \"\") + \"_lag_\" + str(lag)] = \\\n",
    "                dataframe.groupby([\"store_nbr\", \"family\"])['sales']. \\\n",
    "                    transform(lambda x: x.shift(lag).ewm(alpha=alpha).mean())\n",
    "    return dataframe\n",
    "\n",
    "alphas = [0.95, 0.9, 0.8, 0.7, 0.5]\n",
    "lags = [16, 30, 60, 90]\n",
    "\n",
    "a = ewm_features(a, alphas, lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[(a.store_nbr == 1) & (a.family == \"GROCERY I\")].set_index(\"date\")[[\"sales\", \"sales_ewm_alpha_095_lag_16\"]].plot(title = \"STORE 1 - GROCERY I\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2678f7a7303e1dde75d32e3c8360243a421631a917ee1d1b4eb20b319f04e27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
